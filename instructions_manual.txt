A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection

ABSTRACT:
	The cross-validation technique show the how to accurately estimate the performance of   ML models. There are two common methods for this: cross-validation and Bootstrap.
	The authors found that for model selection choosing the best classifier from a set of classifiers. The best method is Ten-Fold Stratified cross-validation, even if more computation power is available. This means that dividing the data into 10 parts and using  each part as a test set once while training on the other a parts is a good way to estimate  model performance .This method is better than using Leave-one -out cross-validation or  more folds in cross-validation.
	This abstract provides helpful guidance on how to accurately evaluate ML models on real world data. 

INTRODUCTION:
Cross-validation is a statistical method that estimates how well a trained model will work on unseen data. The model's efficiency is validated by training it on a subset of input data and testing on a different subset. Cross-validation helps in building a generalized model. Due to the iterative nature of modeling, cross-validation is useful for both performance estimation and model selection.
The three steps involved in cross-validation are:
i. Divide the dataset into two parts: one for training and other for testing.
ii. Train the model with the training dataset.
iii. Evaluate the model's performance using the testing set. If the model doesn't perform well with the testing set, check for issues.
If the model performs well on unseen data, it's consistent and can predict with good accuracy for a wide range of input data; this model is stable. Cross-validation helps evaluate the stability of machine learning models.

Here's a step-by-step process of how cross-validation works:
1.	Data Partitioning: Start with the original dataset, which consists of instances or data points. Divide the dataset into k equal-sized folds (typically 5 or 10) to perform k-fold cross-validation. Alternatively, you can choose another variant such as leave-one-out cross-validation (LOOCV), where each instance serves as a separate fold.
2.	Iteration Loop: Perform the following steps for each iteration of cross-validation:
a.	Fold Selection: Select one fold as the evaluation set or validation set, and the remaining k-1 folds as the training set.
b.	Model Training: Train a model using the training set. The model can be any type of statistical or machine learning model, depending on the problem at hand.
c.	Model Evaluation: Evaluate the trained model's performance on the evaluation set. Calculate the evaluation metric of interest, such as accuracy, mean squared error, or area under the curve (AUC).
d.	Performance Recording: Record the performance metric obtained from the evaluation set for the current iteration.
3.	Aggregation of Performance: Once all iterations are completed, aggregate the performance metrics obtained from each iteration. This aggregation can be done by calculating the average performance across all folds or constructing confidence intervals to assess the variability of the performance estimates.
4.	Final Performance Estimate: The aggregated performance metric provides an estimate of the model's performance on unseen data. It represents the overall performance of the model across different subsets of the data.
	The step-by-step process of cross-validation allows for a comprehensive evaluation of the model's performance by training and testing it on multiple subsets of the data. It helps identify potential issues such as overfitting or underfitting and provides a reliable estimate of how well the model is likely to perform on unseen data.
	It's important to note that there are variations of cross-validation, such as stratified cross-validation and nested cross-validation, which introduce additional steps or considerations. The specific steps can be tailored based on the variant of cross-validation chosen and the requirements of the analysis.

What's the need for cross-validation?

	In order to build a generalized model that works well for unseen data, cross-validation is needed. This is how it's done:
	Split the data into 3 random parts: Training data (65%), Validation data (20%), and Test data (remaining 15%). The model building doesn't involve test data; it is used as 'unseen' data to verify and declare the model accuracy.
	Let's say, a model (kNN) is built using Training data and is optimized (optimum k is arrived at) using Validation data. Only 65% of the entire available data is used for model building, which isn't a good sign. With cross-validation, 85% (Training + Validation data) can be used to build the model. Here's how:
	k-fold concept is applied now by dividing the Training and Validation data into equal parts, say 5 parts of about 17% each. The model is trained 5 times, each time with 17% Validation data and rest 68% Training data. An aggregation mechanism like average value is applied to arrive at the final optimal model. The final model, built with 85% data, is checked for accuracy with Test data. This ensures a 'generalized' model is built that works well for unseen data too

What is the difference between Training Data, Validation Data, and Test Data?
 
For training and testing the model, the dataset must be split into three distinct parts:

	Training Data: The model is trained to learn the hidden features/patterns of the dataset with the training data. The model evaluates the data repeatedly to learn more about the data's behavior, following which, it adjusts itself to serve the intended purpose. It's basically used to fit the models.
	Validation Data: This is used to validate the model performance during training. It helps tune the model's hyper-parameters and configurations accordingly. The validation data estimates the prediction error for model selection. An over-fitting model is prevented with validation data.
	Test Data: After completion of training, the test data validates that the trained model can make accurate predictions. It's used for assessment of the generalization error of the final chosen model.
What are the main assumptions behind cross-validation?

	The learning dataset that is used to build and evaluate a predictive model is assumed to be a sample from the population of interest. With random sub-sampling methods, the training set and test set are generated from the learning set. A supervised prediction method is only expected to learn how to predict on unseen samples that are drawn from the same distribution as training samples; an evaluation of its performance ought to respect this assumption, as in the case of cross-validation with random partitions.
	Random Cross-Validation assumes that a randomly selected set of samples comprising the test set, well represents unseen data. This assumption doesn't hold true when samples are obtained from different experimental conditions.
	
How do we choose k� value for k-fold cross-validation?

The key-configuration parameter for k-fold cross-validation is k� - the number of folds that the given dataset must be split into. Commonly, k�-value is chosen as follows:
Representative: The k�-value is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.
By performing a sensitivity analysis for different k� values, the optimal value can 
be determined. This implies evaluating the performance of the same model on the same dataset for different values of k� and see how they compare.
To compare classifiers with similar bias, k=2�=2 works best as it has lowest 
variance. To measure error, k=5�=5 or k=10�=10 are less biased 
than k=2�=2.Most common values chosen for k� are 3, 5, and 10, with the most popular one being 10, experimentally found to provide good trade-off of low computational cost and low bias in an estimate of model performance.
Typically, low k� values result in a noisy estimate of model performance, while 
large k� values result in a less noisy estimate.
The computation time increases almost exponentially with higher values of k�, particularly with large datasets.

What are the challenges with cross-validation?
	Cross-validation simply provides one additional mapping from training sets to models. Any mapping of this kind constitutes an inductive bias; hence like any other classification strategy, the performance of cross-validation depends on the environment in which it is applied.
	For ideal conditions, it provides optimum output. But with inconsistent data, it may produce drastic result. This is one of the biggest disadvantages of cross-validation as there is no certainty of the type of data in machine learning.
	In predictive modeling, data evolves over a period, and it may face the differences between training set and validation sets. For example, if a model has been created to predict stock market values, by training it on stock values of the previous 5 years, the realistic future values for the next 5 years could be drastically different.
	While k-fold cross-validation is typically the method of choice to estimate the generalization error for small sample sizes, there exists no universal (valid under all distributions) unbiased estimator of the variance of this technique.
Evaluating your Machine Learning Model:
	The primary aim of the Machine Learning model is to learn from the given data and generate predictions based on the pattern observed during the learning process. However, our task doesn’t end there. We need to continuously make improvements to the models, based on the kind of results it generates. We also quantify the model’s performance using metrics like Accuracy, Mean Squared Error(MSE), F1-Score, etc and try to improve these metrics. This can often get tricky when we have to maintain the flexibility of the model without compromising on its correctness.
	A supervised Machine Learning model aims to train itself on the input variables(X) in such a way that the predicted values(Y) are as close to the actual values as possible (Modafinil). This difference between the actual values and predicted values is the error and it is used to evaluate the model. The error for any supervised Machine Learning algorithm comprises of 3 parts:
1.	Bias error
2.	Variance error
3.	The noise
While the noise is the irreducible error that we cannot eliminate, the other two i.e. Bias and Variance are reducible errors that we can attempt to minimize as much as possible.
What is Bias?
	In the simplest terms, Bias is the difference between the Predicted Value and the Expected Value. To explain further, the model makes certain assumptions when it trains on the data provided. When it is introduced to the testing/validation data, these assumptions may not always be correct.
	In our model, if we use a large number of nearest neighbors, the model can totally decide that some parameters are not important at all.  For example, it can just consider that the Glusoce level and the Blood Pressure decide if the patient has diabetes. This model would make very strong assumptions about the other parameters not affecting the outcome. You can also think of it as a model predicting a simple relationship when the datapoints clearly indicate a more complex relationship:
 
Mathematically, let the input variables be X and a target variable Y. We map the relationship between the two using a function f.
Therefore, Y = f(X) + e
Here ‘e’ is the error that is normally distributed. The aim of our model f'(x) is to predict values as close to f(x) as possible. Here, the Bias of the model is:
Bias[f'(X)] = E[f'(X) – f(X)]
As I explained above, when the model makes the generalizations i.e. when there is a high bias error, it results in a very simplistic model that does not consider the variations very well. Since it does not learn the training data very well, it is called Underfitting.


What is a Variance?
	Contrary to bias, the Variance is when the model takes into account the fluctuations in the data i.e. the noise as well. So, what happens when our model has a high variance?
	The model will still consider the variance as something to learn from. That is, the model learns too much from the training data, so much so, that when confronted with new (testing) data, it is unable to predict accurately based on it.
	Mathematically, the variance error in the model is:
	Variance[f(x))=E[X^2]−E[X]^2
	Since in the case of high variance, the model learns too much from the training data, it is called overfitting.
	In the context of our data, if we use very few nearest neighbors, it is like saying that if the number of pregnancies is more than 3, the glucose level is more than 78, Diastolic BP is less than 98, Skin thickness is less than 23 mm and so on for every feature….. decide that the patient has diabetes. All the other patients who don’t meet the above criteria are not diabetic. While this may be true for one particular patient in the training set, what if these parameters are the outliers or were even recorded incorrectly? Clearly, such a model could prove to be very costly!
	Additionally, this model would have a high variance error because the predictions of the patient being diabetic or not vary greatly with the kind of training data we are providing it. So even changing the Glucose Level to 75 would result in the model predicting that the patient does not have diabetes.
	To make it simpler, the model predicts very complex relationships between the outcome and the input features when a quadratic equation would have sufficed. This is how a classification model would look like when there is a high variance error/when there is overfitting:

 

To summarise,
•	A model with a high bias error underfits data and makes very simplistic assumptions on it
•	A model with a high variance error overfits the data and learns too much from it
•	A good model is where both Bias and Variance errors are balanced
 
Bias-Variance Tradeoff:
	How do we relate the above concepts to our Knn model from earlier? Let’s find out!
	In our model, say, for, k = 1, the point closest to the datapoint in question will be considered. Here, the prediction might be accurate for that particular data point so the bias error will be less.
	However, the variance error will be high since only the one nearest point is considered and this doesn’t take into account the other possible points. What scenario do you think this corresponds to? Yes, you are thinking right, this means that our model is overfitting.
	On the other hand, for higher values of k, many more points closer to the datapoint in question will be considered. This would result in higher bias error  and underfitting since many points closer to the datapoint are considered and thus it can’t learn the specifics from the training set. However, we can account for a lower variance error for the testing set which has unknown values.
	To achieve a balance between the Bias error and the Variance error, we need a value of k such that the model neither learns from the noise (overfit on data) nor makes sweeping assumptions on the data(underfit on data). To keep it simpler, a balanced model would look like this:
 
Though some points are classified incorrectly, the model generally fits most of the datapoints accurately. The balance between the Bias error and the Variance error is the Bias-Variance Tradeoff.
The following bulls-eye diagram explains the tradeoff better:
 
The center i.e. the bull’s eye is the model result we want to achieve that perfectly predicts all the values correctly. As we move away from the bull’s eye, our model starts to make more and more wrong predictions.
A model with low bias and high variance predicts points that are around the center generally, but pretty far away from each other. A model with high bias and low variance is pretty far away from the bull’s eye, but since the variance is low, the predicted points are closer to each other.
In terms of model complexity, we can use the following diagram to decide on the optimal complexity of our model.
 
So, what do you think is the optimum value for k?
From the above explanation, we can conclude that the k for which
•	the testing score is the highest, and
•	both the test score and the training score are close to each other
is the optimal value of k. So, even though we are compromising on a lower training score, we still get a high score for our testing data which is more crucial – the test data is after all unknown data.
Let us make a table for different values of k to further prove this:
 
Conclusion
To summarize, in this article, we learned that an ideal model would be one where both the bias error and the variance error are low. However, we should always aim for a model where the model score for the training data is as close as possible to the model score for the testing data.
That’s where we figured out how to choose a model that is not too complex (High variance and low bias) which would lead to overfitting and nor too simple(High Bias and low variance) which would lead to underfitting.
Bias and Variance plays an important role in deciding which predictive model to use.

CROSS_VALIDATION TECHNIQUES:

Types of Cross Validation
There are different types of cross validation methods, and they could be classified into two broad categories – Non-exhaustive and Exhaustive Methods.
	Non-exhaustive Methods
Non-exhaustive cross validation methods, as the name suggests do not compute all ways of splitting the original data. Let us go through the methods to get a clearer understanding.
	Exhaustive Methods
Exhaustive cross validation methods and test on all possible ways to divide the original sample into a training and a validation set.
Non-exhausted methods:
1.	Hold Out Cross-validation
2.	K-Fold cross-validation
3.	Stratified K-Fold cross-validation
Exhaustive Methods:
1.	Leave Pout Cross-validation 
2.	Leave One Out Cross-validation
3.	Monte Carlo (Shuffle-Split)
4.	Time Series ( Rolling cross-validation)



CROSS_VALIDATION TECHNIQUES:

1.HoldOut Cross-validation or Train-Test Split
In this technique of cross-validation, the whole dataset is randomly partitioned into a training set and validation set. Using a rule of thumb nearly 70% of the whole dataset is used as a training set and the remaining 30% is used as the validation set.
 
Pros:
1. Quick To Execute: As we have to split the dataset into training and validation set just once and the model will be built just once on the training set so gets executed quickly.
Cons:
1. Not Suitable for an imbalanced dataset: Suppose we have an imbalanced dataset that has class ‘0’ and class ‘1’. Let’s say 80% of data belongs to class ‘0’ and the remaining 20% data to class ‘1’.On doing train-test split with train set size as 80% and test data size as 20% of the dataset. It may happen that all 80% data of class ‘0’ may be in the training set and all data of class ‘1’ in the test set. So our model will not generalize well for our test data as it hasn’t seen data of class ‘1’ before.
2. A large chunk of data gets deprived of training the model.
In the case of a small dataset, a part will be kept aside for testing the model which may have important characteristics which our model may miss out on as it has not trained on that data.
2. K-Fold Cross-Validation
In this technique of K-Fold cross-validation, the whole dataset is partitioned into K parts of equal size. Each partition is called a “Fold“.So as we have K parts we call it K-Folds. One Fold is used as a validation set and the remaining K-1 folds are used as the training set.
The technique is repeated K times until each fold is used as a validation set and the remaining folds as the training set.
The final accuracy of the model is computed by taking the mean accuracy of the k-models validation data.
k-Fold cross-validation is a technique that minimizes the disadvantages of the hold-out method. k-Fold introduces a new way of splitting the dataset which helps to overcome the “test only once bottleneck”.
The algorithm of the k-Fold technique:
1.	Pick a number of folds – k. Usually, k is 5 or 10 but you can choose any number which is less than the dataset’s length.
2.	Split the dataset into k equal (if possible) parts (they are called folds)
3.	Choose k – 1 folds as the training set. The remaining fold will be the test set
4.	Train the model on the training set. On each iteration of cross-validation, you must train a new model independently of the model trained on the previous iteration
5.	Validate on the test set
6.	Save the result of the validation
7.	Repeat steps 3 – 6 k times. Each time use the remaining  fold as the test set. In the end, you should have validated the model on every fold that you have.
8.	To get the final score average the results that you got on step 6.

 
Pros:
1. The whole dataset is used as both a training set and validation set:
Cons:
1. Not to be used for imbalanced datasets: As discussed in the case of HoldOut cross-validation, in the case of K-Fold validation too it may happen that all samples of training set will have no sample form class “1” and only of class “0”.And the validation set will have a sample of class “1”.
2. Not suitable for Time Series data: For Time Series data the order of the samples matter. But in K-Fold Cross-Validation, samples are selected in random order.
3. Stratified K-Fold Cross-Validation
Stratified K-Fold is an enhanced version of K-Fold cross-validation which is mainly used for imbalanced datasets. Just like K-fold, the whole dataset is divided into K-folds of equal size.
But in this technique, each fold will have the same ratio of instances of target variable as in the whole datasets.
 
Pros:
1. Works perfectly well for Imbalanced Data: Each fold in stratified cross-validation will have a representation of data of all classes in the same ratio as in the whole dataset.
Cons:
1. Not suitable for Time Series data: For Time Series data the order of the samples matter. But in Stratified Cross-Validation, samples are selected in random order.
4. Leave P Out cross-validation
LeavePOut cross-validation is an exhaustive cross-validation technique, in which p-samples are used as the validation set and remaining n-p samples are used as the training set.
Suppose we have 100 samples in the dataset. If we use p=10 then in each iteration 10 values will be used as a validation set and the remaining 90 samples as the training set.
This process is repeated till the whole dataset gets divided on the validation set of p-samples and n-p training samples.
Pros:
All the data samples get used as both training and validation samples.
Cons:
1. High computation time: As the above technique will keep on repeating until all samples get used up as a validation set, it will have higher computational time.
2. Not Suitable for Imbalanced dataset: Same as in K-Fold Cross-validation, if in the training set we have samples of only 1 class then our model will not be able to generalize for the validation set.
5. Leave One Out cross-validation
LeaveOneOut cross-validation is an exhaustive cross-validation technique in which 1 sample point is used as a validation set and the remaining n-1 samples are used as the training set.
Suppose we have 100 samples in the dataset. Then in each iteration 1 value will be used as a validation set and the remaining 99 samples as the training set. Thus the process is repeated till every sample of the dataset is used as a validation point.
It is the same as LeavePOut cross-validation with p=1.
 

 
6. Monte Carlo Cross-Validation(Shuffle Split)
Monte Carlo cross-validation, also known as Shuffle Split cross-validation, is a very flexible strategy of cross-validation. In this technique, the datasets get randomly partitioned into training and validation sets.
We have decided upon the percentage of the dataset we want to be used as a training set and the percentage to be used as a validation set. If the added percentage of training and validation set size is not sum up to 100 then the remaining dataset is not used in either training or validation set.
Let’s say we have 100 samples and 60% of samples to be used as training set and 20% of the sample to be used as validation set then the remaining 20%( 100-(60+20)) is not to be used.
This splitting will be repeated ‘n’ times that we have to specify.
  
Pros:
1. We are free to use the size of the training and validation set.
2. We can choose the number of repetitions and not depend on the number of folds for repetitions.
Cons:
1. Few samples may not be selected for either training or validation set.
2. Not Suitable for Imbalanced datasets: After we define the size of the training set and validation set, all the samples are randomly selected, so it may happen that the training set may don’t have the class of data that is in the test set, and the model won’t be able to generalize for unseen data.
7. Time Series Cross-Validation
What is a Time Series Data?
Time series data is data that is collected at different points in time. As the data points are collected at adjacent time periods there is potential for correlation between observations. This is one of the features that distinguishes time-series data from cross-sectional data.
How cross-validation is done in the case of Time-series data?
In the case of time-series data, we cannot choose random samples and assign them to either training or validation set as it makes no sense in using the values from the future data to predict values of the past data.
As the order of the data is very important for time series related problems, so we split the data into training and validation set according to time, also called as “Forward chaining” method or rolling cross-validation.
We start with a small subset of data as the training set. Based on that set we predict later data points and then check the accuracy.
The Predicted samples are then included as part of the next training dataset and subsequent samples are forecasted.
  
Pros:
One of the finest techniques .
Cons:
Not suitable for validation of other data types: As in other techniques we choose random samples as training or validation set, but in this technique order of data is very important.


WHAT IS BOOTSTRAP ?
In cross-validation, bootstrap is a resampling technique used to estimate the accuracy of a model. It involves randomly sampling the available dataset with replacement to create multiple bootstrap samples. Each sample is used to train a model, and the accuracy of the model is then evaluated on the remaining instances not included in the training set. By repeating this process multiple times and aggregating the accuracy scores, we can obtain an estimate of the model's accuracy. Bootstrap helps address limitations in data by creating simulated datasets and provides a more comprehensive assessment of the model's performance.
 
In cross-validation, bootstrap is a resampling technique used to estimate the performance or evaluate the stability of a statistical model. It is particularly useful when the available dataset is limited or when there is a desire to assess the variability of the model's performance.
The Algorithm To Create One Sample
N = size of Dataset (population)
n = target size of sample
cn = current size of the sample
s = sample

While (cn < n)
  Randomly select an item i from the dataset D 
  Add the randomly selected item i to s. 
  cn++
Here's how bootstrap works in the context of cross-validation:
Data Sampling: In a traditional cross-validation procedure, the dataset is split into multiple subsets or folds. In bootstrap, instead of creating distinct folds, a random sample is drawn from the original dataset with replacement. This means that each bootstrap sample may contain duplicate instances as well as missing instances from the original dataset.
Model Fitting: For each bootstrap sample, a model is trained on the selected sample. The model can be any type of statistical or machine learning model, depending on the problem at hand.
Model Evaluation: After training the model on each bootstrap sample, the model's performance is evaluated on the remaining instances, which were not included in the training set. This evaluation step is typically done using a predefined evaluation metric, such as accuracy, mean squared error, or area under the curve (AUC).
Aggregation: Finally, the performance metrics obtained from each bootstrap sample are aggregated to provide an overall estimate of the model's performance. This aggregation can be done by calculating the average performance across all bootstrap samples or by constructing a confidence interval to assess the variability of the performance estimates.
The bootstrap method helps to address issues related to limited data by generating multiple simulated datasets from the original data, allowing for a more comprehensive evaluation of the model's performance. It can also provide insights into the stability and robustness of the model by examining the variability of the performance estimates across the bootstrap samples.
It's worth noting that bootstrap can be used in conjunction with various cross-validation techniques, such as k-fold cross-validation or leave-one-out cross-validation, to further enhance the estimation of model performance.


Here's a detailed step-by-step explanation of how the bootstrap method works:

	Original Dataset: Start with the original dataset, which consists of a set of instances or data points.
	Sampling with Replacement: Randomly select a subset of instances from the original dataset, allowing for duplicates in the selection. This means that an instance can be selected multiple times or not selected at all. The size of the bootstrap sample is typically the same as the size of the original dataset, but it can be smaller or larger depending on the specific application.
	Training Set: The selected instances form the training set for this iteration of the bootstrap method. Note that some instances from the original dataset may not be included in the training set, while others may be duplicated.
	Model Training: Train a model on the current bootstrap training set. The model can be any type of statistical or machine learning model suitable for the problem at hand, such as linear regression, decision trees, or neural networks.
	Model Evaluation: Evaluate the performance of the trained model on the instances that were not included in the bootstrap training set. These instances are often referred to as the "out-of-bag" (OOB) samples. Compute the evaluation metric of interest, such as accuracy, precision, recall, or mean squared error, using the model's predictions on the OOB samples.
	Iteration: Repeat steps 2-5 multiple times (usually a large number of iterations, such as 100 or 1000) to create multiple bootstrap samples, train corresponding models, and evaluate their performance.
	Aggregation: Once all iterations are completed, aggregate the performance metrics obtained from each iteration. This aggregation can be done by calculating the average performance across all bootstrap samples or by constructing a confidence interval to capture the variability of the performance estimates.
	Performance Estimate: The aggregated performance metric provides an estimate of the model's performance on unseen data. It represents the average performance over the bootstrap samples or provides an interval that quantifies the uncertainty associated with the estimate.

By repeating the process described above, the bootstrap method allows us to estimate the performance of a model and gain insights into its stability and variability. It is particularly useful when the available dataset is limited, as it generates multiple simulated datasets and provides a more comprehensive assessment of the model's performance.

Future enchancement for crossvalidation and bootstrap for model selection and accuracy estimation. 
In the realm of model selection and accuracy estimation, cross-validation and bootstrap are widely used techniques. While they have proven to be effective, there are several potential future enhancements that can further improve their application. Here are some possible future enhancements for cross-validation and bootstrap:

	Advanced Resampling Techniques: Researchers are continuously developing more sophisticated resampling techniques beyond traditional cross-validation and bootstrap. These advanced techniques aim to capture more nuanced patterns in the data and provide more accurate model selection and estimation. Examples include stratified cross-validation, leave-one-out cross-validation, and time series cross-validation.
	Model-Specific Resampling: Tailoring the resampling approach to specific model types can be beneficial. For instance, incorporating the characteristics of the model, such as its assumptions or structure, into the resampling process can lead to better performance. This can involve adapting the sampling strategy or modifying the evaluation metrics based on the model being used.
	Ensemble Resampling: Ensembles combine multiple models to improve overall performance. Similarly, ensemble resampling techniques can leverage the power of multiple resampling iterations to enhance model selection and accuracy estimation. These techniques involve aggregating results from various resampling runs, potentially using methods like bagging or boosting, to obtain more robust estimates.
	Incorporating Uncertainty Measures: Cross-validation and bootstrap can provide point estimates of model performance, but they often don't capture the uncertainty associated with these estimates. Future enhancements could focus on developing methodologies to quantify and incorporate uncertainty measures into the model selection and accuracy estimation process. This would provide more comprehensive information about the reliability of the results.
	Bayesian Approaches: Bayesian methods offer a probabilistic framework for modeling and inference. Future enhancements could explore the integration of Bayesian approaches with cross-validation and bootstrap techniques. Bayesian cross-validation and bootstrap can provide more principled and flexible approaches to model selection and estimation, taking into account prior knowledge, hyperparameter uncertainty, and model uncertainty.
	Parallelization and Scalability: As datasets continue to grow in size, it becomes essential to scale up the model selection and accuracy estimation procedures. Future enhancements could focus on developing parallelized implementations of cross-validation and bootstrap techniques, making them more efficient and capable of handling large-scale data.
	Adaptive Sampling: Adaptive sampling techniques dynamically adjust the sampling strategy based on the evolving characteristics of the data. Future enhancements might explore adaptive cross-validation and bootstrap methods that intelligently adapt the resampling procedure to optimize model selection and accuracy estimation for different data patterns and distributions.
•	These potential future enhancements aim to refine and extend the existing cross-validation and bootstrap techniques, making them more robust, accurate, and applicable to a wide range of modeling scenarios. While some of these enhancements are already being explored in research, further advancements and improvements are likely to emerge as the field continues to evolve.
 Bootstrapping or Cross-Validation
1.	Bootstrapping selects samples with replacements that can be as big as the dataset.
2.	Cross-validation samples are smaller than the dataset.
3.	Bootstrapping contains repeated elements in every subset. Bootstrapping relies on random sampling.
4.	Cross-validation does not rely on random sampling, just splitting the dataset into k unique subsets.
5.	Cross-validation is usually used to test an ML model's generalization capabilities.
6.	Bootstrapping is used more for statistical tests, ensemble machine learning, and parameter estimation.
If you are still not sure which one to use for testing your ML model, just go with cross-validation.
